{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MVTorch](https://github.com/ajhamdi/mvtorch) 3D Part Segmentation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- install `mvtorch` from [here](https://github.com/ajhamdi/mvtorch/blob/main/INSTALL.md) and activate the environment in the notebook.\n",
    "\n",
    "- download common 3D datasets ([ShapeNet Parts](https://shapenet.cs.stanford.edu/media/shapenet_part_seg_hdf5_data.zip)) and unzip inside `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd .. && cd .. && mkdir data && cd data/ && wget https://shapenet.cs.stanford.edu/media/shapenet_part_seg_hdf5_data.zip --no-check-certificate # download ShapeNet Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depenenancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from mvtorch.data import ShapeNetPart, CustomDataLoader\n",
    "from mvtorch.view_selector import MVTN, MVLiftingModule\n",
    "from mvtorch.mvrenderer import MVRenderer\n",
    "from mvtorch.networks import MLPClassifier, MVNetwork\n",
    "from mvtorch.ops import svctomvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='../../data/hdf5_data' # specifiy where did you put the data relative to notebook\n",
    "nb_views = 1 # Number of views generated by view selector\n",
    "nb_epochs = 100 # Number of epochs\n",
    "parallel_head = True # Do segmentation with parallel heads where each head is focused on one class\n",
    "lambda_l2d = 1 # The 2D CE loss coefficient in the segmentation pipeline\n",
    "lambda_l3d = 0 # The 3D CE loss coefficient on the segmentation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_train = ShapeNetPart(root_dir=data_dir, split='trainval')\n",
    "dset_test = ShapeNetPart(root_dir=data_dir, split='test')\n",
    "train_loader = CustomDataLoader(dset_train, batch_size=5, shuffle=True, drop_last=True)\n",
    "test_loader = CustomDataLoader(dset_test, batch_size=5, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define main components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/hamdiaj/.cache/torch/hub/pytorch_vision_v0.8.2\n"
     ]
    }
   ],
   "source": [
    "# Create backbone multi-view network (DeepLabV3 with ResNet-101)\n",
    "num_classes = len(dset_train.cat2id)\n",
    "num_parts = max(dset_train.seg_num)\n",
    "mvnetwork = MVNetwork(num_classes=num_classes, num_parts=num_parts, mode='part', net_name='deeplab').cuda()\n",
    "\n",
    "# Create backbone optimizer\n",
    "optimizer = torch.optim.AdamW(mvnetwork.parameters(), lr=0.00001, weight_decay=0.03)\n",
    "\n",
    "# Create view selector\n",
    "mvtn = MVTN(nb_views=nb_views).cuda()\n",
    "\n",
    "# Create optimizer for view selector (In case views are not fixed, otherwise set to None)\n",
    "# mvtn_optimizer = torch.optim.AdamW(mvtn.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "mvtn_optimizer = None\n",
    "\n",
    "# Create multi-view renderer\n",
    "mvrenderer = MVRenderer(nb_views=nb_views, return_mapping=True)\n",
    "\n",
    "# Create the MLP classifier\n",
    "mlp_classifier = MLPClassifier(num_classes=num_classes, num_parts=num_parts)\n",
    "\n",
    "# Create the multi-view lifting module\n",
    "mvlifting = MVLiftingModule(image_size=224, lifting_method='mode', mlp_classifier=mlp_classifier, balanced_object_loss=True, balanced_3d_loss_alpha=0, lifting_net=None, use_early_voint_feats=False).cuda()\n",
    "\n",
    "# Create loss function for training\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## util function for unprojection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate 2D and 3D loss as well as the 3D predictions\n",
    "def calc_loss_and_3d_pred(rendered_images, cls, labels_2d, azim, elev, points, indxs, parts_nb, view_info, seg):\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "    outputs, feats = mvnetwork(rendered_images, cls)\n",
    "    loss2d = mvnetwork.get_loss(criterion, outputs, labels_2d, cls)\n",
    "    views_weights = mvlifting.compute_views_weights(azim, elev, rendered_images, normals=None) \n",
    "    predictions_3d = mvlifting.lift_2D_to_3D(points, predictions_2d=svctomvc(outputs, nb_views=nb_views), rendered_pix_to_point=indxs, views_weights=views_weights, cls=cls, parts_nb=parts_nb, view_info=view_info, early_feats=feats)\n",
    "    criterion3d = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "    loss3d = mvlifting.get_loss_3d(criterion3d, predictions_3d, seg, cls)\n",
    "    loss = lambda_l2d * loss2d + lambda_l3d * loss3d\n",
    "    return loss, predictions_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n",
      "Training...\n",
      "\tBatch 700/2801: Current Average Training Loss = 1.35458\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m labels_2d, pix_to_face_mask \u001b[38;5;241m=\u001b[39m mvlifting\u001b[38;5;241m.\u001b[39mcompute_image_segment_label_points(points, batch_points_labels\u001b[38;5;241m=\u001b[39mseg, rendered_pix_to_point\u001b[38;5;241m=\u001b[39mindxs)\n\u001b[1;32m     19\u001b[0m labels_2d, rendered_images \u001b[38;5;241m=\u001b[39m labels_2d\u001b[38;5;241m.\u001b[39mcuda(), rendered_images\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 21\u001b[0m loss, predictions_3d \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_and_3d_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrendered_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mazim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparts_nb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m _, predictions_3d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(predictions_3d, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m, in \u001b[0;36mcalc_loss_and_3d_pred\u001b[0;34m(rendered_images, cls, labels_2d, azim, elev, points, indxs, parts_nb, view_info, seg)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_loss_and_3d_pred\u001b[39m(rendered_images, \u001b[38;5;28mcls\u001b[39m, labels_2d, azim, elev, points, indxs, parts_nb, view_info, seg):\n\u001b[1;32m      3\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     outputs, feats \u001b[38;5;241m=\u001b[39m \u001b[43mmvnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrendered_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     loss2d \u001b[38;5;241m=\u001b[39m mvnetwork\u001b[38;5;241m.\u001b[39mget_loss(criterion, outputs, labels_2d, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m      6\u001b[0m     views_weights \u001b[38;5;241m=\u001b[39m mvlifting\u001b[38;5;241m.\u001b[39mcompute_views_weights(azim, elev, rendered_images, normals\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \n",
      "File \u001b[0;32m~/anaconda3/envs/mvtorchenv1/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/mvtorchenv1/lib/python3.9/site-packages/mvtorch/networks.py:49\u001b[0m, in \u001b[0;36mMVNetwork.forward\u001b[0;34m(self, mvimages, cls)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmvnetwork(mvimages)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpart\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmvnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmvimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mvtorchenv1/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/mvtorchenv1/lib/python3.9/site-packages/mvtorch/view_selector.py:472\u001b[0m, in \u001b[0;36mMVPartSegmentation.forward\u001b[0;34m(self, mvimages, cls)\u001b[0m\n\u001b[1;32m    468\u001b[0m     logits_all_shapes\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_shape_heads[ii](features)[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m    470\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(logits_all_shapes, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m#######\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m target_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mcls\u001b[39m\n\u001b[1;32m    473\u001b[0m predict_mask \u001b[38;5;241m=\u001b[39m target_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, nb_views, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    474\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs \u001b[38;5;241m*\u001b[39m rearrange(predict_mask, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb V C h w cls -> (b V) C h w cls\u001b[39m\u001b[38;5;124m'\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(nb_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{nb_epochs}\")\n",
    "\n",
    "    print(\"Training...\")\n",
    "    mvnetwork.train()\n",
    "    mvtn.train()\n",
    "    mvrenderer.train()\n",
    "    running_loss = 0\n",
    "    for i, (points, cls, seg, parts_range, parts_nb, _) in enumerate(train_loader):\n",
    "        azim, elev, dist = mvtn(points, c_batch_size=len(points))\n",
    "        view_info = torch.cat([azim.unsqueeze(-1), elev.unsqueeze(-1)], dim=-1)\n",
    "        rendered_images, indxs, distance_weight_maps, _ = mvrenderer(None, points, azim=azim, elev=elev, dist=dist, color=None)\n",
    "\n",
    "        cls, seg, points = cls.cuda(), seg.cuda(), points.cuda()\n",
    "\n",
    "        seg = seg + 1 - parts_range[..., None].cuda().to(torch.int) if parallel_head else seg + 1\n",
    "\n",
    "        labels_2d, pix_to_face_mask = mvlifting.compute_image_segment_label_points(points, batch_points_labels=seg, rendered_pix_to_point=indxs)\n",
    "        labels_2d, rendered_images = labels_2d.cuda(), rendered_images.cuda()\n",
    "\n",
    "        loss, predictions_3d = calc_loss_and_3d_pred(rendered_images, cls, labels_2d, azim, elev, points, indxs, parts_nb, view_info, seg)\n",
    "        _, predictions_3d = torch.max(predictions_3d, dim=1)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if mvtn_optimizer is not None:\n",
    "            mvtn_optimizer.step()\n",
    "            mvtn_optimizer.zero_grad()\n",
    "        \n",
    "        if (i + 1) % int(len(train_loader) * 0.25) == 0:\n",
    "            print(f\"\\tBatch {i + 1}/{len(train_loader)}: Current Average Training Loss = {(running_loss / (i + 1)):.5f}\")\n",
    "    print(f\"Total Average Training Loss = {(running_loss / len(train_loader)):.5f}\")\n",
    "\n",
    "    print(\"Testing...\")\n",
    "    mvnetwork.eval()\n",
    "    mvtn.eval()\n",
    "    mvrenderer.eval()\n",
    "    running_loss = 0\n",
    "    for i, (points, cls, seg, parts_range, parts_nb, _) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            azim, elev, dist = mvtn(points, c_batch_size=len(points))\n",
    "            view_info = torch.cat([azim.unsqueeze(-1), elev.unsqueeze(-1)], dim=-1)\n",
    "            rendered_images, indxs, distance_weight_maps, _ = mvrenderer(None, points, azim=azim, elev=elev, dist=dist, color=None)\n",
    "\n",
    "            cls, seg, points = cls.cuda(), seg.cuda(), points.cuda()\n",
    "\n",
    "            seg = seg + 1 - parts_range[..., None].cuda().to(torch.int)\n",
    "            parts_range += 1\n",
    "\n",
    "            labels_2d, pix_to_face_mask = mvlifting.compute_image_segment_label_points(points, batch_points_labels=seg, rendered_pix_to_point=indxs)\n",
    "\n",
    "            loss, _ = calc_loss_and_3d_pred(rendered_images, cls, labels_2d, azim, elev, points, indxs, parts_nb, view_info, seg)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % int(len(test_loader) * 0.25) == 0:\n",
    "                print(f\"\\tBatch {i + 1}/{len(test_loader)}: Current Average Test Loss = {(running_loss / (i + 1)):.5f}\")\n",
    "    print(f\"Total Average Test Loss = {(running_loss / len(test_loader)):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mvtorchenv1]",
   "language": "python",
   "name": "conda-env-mvtorchenv1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
