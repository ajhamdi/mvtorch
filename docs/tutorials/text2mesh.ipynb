{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MVTorch](https://github.com/ajhamdi/mvtorch) Text to 3D Mesh Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- install `mvtorch` from [here](https://github.com/ajhamdi/mvtorch/blob/main/INSTALL.md) and activate the environment in the notebook.\n",
    "\n",
    "- multiple mesh templates are available for test are [here](https://github.com/ajhamdi/mvtorch/tree/main/data/meshes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depenenancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "import torchvision\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from mvtorch.utils import torch_color\n",
    "from mvtorch.mvrenderer import MVRenderer\n",
    "from mvtorch.models.text2mesh import Mesh, MeshNormalizer, NeuralStyleField, device\n",
    "from pytorch3d.structures import Meshes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_path = '../../data/meshes/candle.obj' # specifiy where did you put the data relative to notebook\n",
    "out_dir = './results/meshes' # specifiy where ou want results to be saved relative to notebook\n",
    "prompt = 'a candle made of colorful wood'\n",
    "clipmodel = 'ViT-B/32'\n",
    "n_augs = 1\n",
    "background = torch_color('white').to(device)\n",
    "cropforward = False\n",
    "normmincrop = 0.1\n",
    "normmaxcrop = 0.1\n",
    "mincrop = 1\n",
    "maxcrop = 1\n",
    "cropsteps = 0\n",
    "n_iter = 1500\n",
    "input_normals = False\n",
    "only_z = False\n",
    "sigma = 5.0\n",
    "depth = 4\n",
    "width = 256\n",
    "colordepth = 2\n",
    "normdepth = 2\n",
    "normratio = 0.1\n",
    "clamp = 'tanh'\n",
    "normclamp = 'tanh'\n",
    "pe = True\n",
    "exclude = 0\n",
    "learning_rate = 0.0005\n",
    "decay = 0\n",
    "lr_decay = 0.9\n",
    "decay_step = 100\n",
    "lr_plateau = False\n",
    "symmetry = False\n",
    "standardize = False\n",
    "n_views = 5\n",
    "show = False\n",
    "clipavg = 'view'\n",
    "splitnormloss = False\n",
    "splitcolorloss = False\n",
    "geoloss = True\n",
    "n_normaugs = 4\n",
    "decayfreq = None\n",
    "cropdecay = 1.0\n",
    "frontview_center = [1.96349, 0.6283] # azim, elev\n",
    "frontview_std = 4\n",
    "render_dist = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_process(out_dir, i, loss, loss_check, losses, rendered_images):\n",
    "    print('iter: {} loss: {}'.format(i, loss.item()))\n",
    "    torchvision.utils.save_image(rendered_images, os.path.join(out_dir, 'iter_{}.jpg'.format(i)))\n",
    "    if lr_plateau and loss_check is not None:\n",
    "        new_loss_check = np.mean(losses[-100:])\n",
    "        # If avg loss increased or plateaued then reduce LR\n",
    "        if new_loss_check >= loss_check:\n",
    "            for g in torch.optim.param_groups:\n",
    "                g['lr'] *= 0.5\n",
    "        loss_check = new_loss_check\n",
    "\n",
    "    elif lr_plateau and loss_check is None and len(losses) >= 100:\n",
    "        loss_check = np.mean(losses[-100:])\n",
    "\n",
    "def export_final_results(out_dir, losses, mesh, mlp, network_input, vertices):\n",
    "    with torch.no_grad():\n",
    "        pred_rgb, pred_normal = mlp(network_input)\n",
    "        pred_rgb = pred_rgb.detach().cpu()\n",
    "        pred_normal = pred_normal.detach().cpu()\n",
    "\n",
    "        torch.save(pred_rgb, os.path.join(out_dir, f\"colors_final.pt\"))\n",
    "        torch.save(pred_normal, os.path.join(out_dir, f\"normals_final.pt\"))\n",
    "\n",
    "        base_color = torch.full(size=(mesh.vertices.shape[0], 3), fill_value=0.5)\n",
    "        final_color = torch.clamp(pred_rgb + base_color, 0, 1)\n",
    "\n",
    "        mesh.vertices = vertices.detach().cpu() + mesh.vertex_normals.detach().cpu() * pred_normal\n",
    "\n",
    "        objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
    "        mesh.export(os.path.join(out_dir, f\"{objbase}_final.obj\"), color=final_color)\n",
    "\n",
    "        # Save final losses\n",
    "        torch.save(torch.tensor(losses), os.path.join(out_dir, \"losses.pt\"))\n",
    "\n",
    "\n",
    "def update_mesh(mlp, network_input, prior_color, sampled_mesh, vertices):\n",
    "    pred_rgb, pred_normal = mlp(network_input)\n",
    "    sampled_mesh.face_attributes = prior_color + pred_rgb\n",
    "    sampled_mesh.vertices = vertices + sampled_mesh.vertex_normals * pred_normal\n",
    "    MeshNormalizer(sampled_mesh)()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CLIP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, preprocess = clip.load(clipmodel, device)\n",
    "res = clip_model.visual.input_resolution\n",
    "    \n",
    "objbase, extension = os.path.splitext(os.path.basename(obj_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MV Renderer and Augmenter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamdiaj/anaconda3/envs/mvtorchenv1/lib/python3.9/site-packages/pytorch3d/io/obj_io.py:542: UserWarning: No mtl file provided\n",
      "  warnings.warn(\"No mtl file provided\")\n"
     ]
    }
   ],
   "source": [
    "render = MVRenderer(n_views, image_size=res, pc_rendering=False, background_color=background, object_color='custom', return_mapping=False)\n",
    "mesh = Mesh(obj_path)\n",
    "MeshNormalizer(mesh)()\n",
    "\n",
    "prior_color = torch.full(size=(mesh.vertices.shape[0], 3), fill_value=0.5, device=device)\n",
    "\n",
    "losses = []\n",
    "\n",
    "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "# CLIP Transform\n",
    "clip_transform = transforms.Compose([\n",
    "    transforms.Resize((res, res)),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "# Augmentation settings\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
    "    transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "# Augmentations for normal network\n",
    "if cropforward :\n",
    "    curcrop = normmincrop\n",
    "else:\n",
    "    curcrop = normmaxcrop\n",
    "normaugment_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(res, scale=(curcrop, curcrop)),\n",
    "    transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
    "    clip_normalizer\n",
    "])\n",
    "cropiter = 0\n",
    "cropupdate = 0\n",
    "if normmincrop < normmaxcrop and cropsteps > 0:\n",
    "    cropiter = round(n_iter / (cropsteps + 1))\n",
    "    cropupdate = (maxcrop - mincrop) / cropiter\n",
    "\n",
    "    if not cropforward:\n",
    "        cropupdate *= -1\n",
    "\n",
    "# Displacement-only augmentations\n",
    "displaugment_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(res, scale=(normmincrop, normmincrop)),\n",
    "    transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "normweight = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare MLP Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Settings\n",
    "input_dim = 6 if input_normals else 3\n",
    "if only_z:\n",
    "    input_dim = 1\n",
    "mlp = NeuralStyleField(sigma, depth, width, 'gaussian', colordepth, normdepth,\n",
    "                            normratio, clamp, normclamp, niter=n_iter,\n",
    "                            progressive_encoding=pe, input_dim=input_dim, exclude=exclude).to(device)\n",
    "mlp.reset_weights()\n",
    "\n",
    "optim = torch.optim.Adam(mlp.parameters(), learning_rate, weight_decay=decay)\n",
    "activate_scheduler = lr_decay < 1 and decay_step > 0 and not lr_plateau\n",
    "if activate_scheduler:\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=decay_step, gamma=lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode and Prepare Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ' '.join(prompt)\n",
    "prompt_token = clip.tokenize([prompt]).to(device)\n",
    "encoded_text = clip_model.encode_text(prompt_token)\n",
    "\n",
    "# Save prompt\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "with open(os.path.join(out_dir, prompt), \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "# Same with normprompt\n",
    "norm_encoded = encoded_text\n",
    "\n",
    "\n",
    "loss_check = None\n",
    "vertices = copy.deepcopy(mesh.vertices)\n",
    "network_input = copy.deepcopy(vertices)\n",
    "if symmetry:\n",
    "    network_input[:,2] = torch.abs(network_input[:,2])\n",
    "\n",
    "if standardize:\n",
    "    # Each channel into z-score\n",
    "    network_input = (network_input - torch.mean(network_input, dim=0))/torch.std(network_input, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimzation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                         | 1/1500 [00:00<19:39,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 loss: -0.2232666015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▋                                    | 101/1500 [00:42<09:15,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100 loss: -0.2210693359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████▏                                 | 201/1500 [01:22<08:54,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 200 loss: -0.2303466796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████▊                               | 301/1500 [02:01<08:08,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 300 loss: -0.253662109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████▍                            | 401/1500 [02:42<07:31,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 400 loss: -0.246826171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████                          | 501/1500 [03:25<06:55,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 500 loss: -0.260009765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████▋                       | 601/1500 [04:06<06:07,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 600 loss: -0.256591796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|██████████████████▏                    | 701/1500 [04:47<05:35,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 700 loss: -0.258544921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|████████████████████▏                  | 777/1500 [05:19<04:52,  2.47it/s]"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(n_iter)):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    sampled_mesh = mesh\n",
    "\n",
    "    update_mesh(mlp, network_input, prior_color, sampled_mesh, vertices)\n",
    "\n",
    "    elev = torch.cat((torch.tensor([frontview_center[1]]), torch.randn(n_views - 1) * np.pi / frontview_std + frontview_center[1]))\n",
    "    azim = torch.cat((torch.tensor([frontview_center[0]]), torch.randn(n_views - 1) * 2 * np.pi / frontview_std + frontview_center[0]))\n",
    "    dist = torch.ones((n_views), dtype=torch.float) * render_dist\n",
    "    color = sampled_mesh.face_attributes\n",
    "    azim, elev, dist, color = azim.unsqueeze(0), elev.unsqueeze(0), dist.unsqueeze(0), color.unsqueeze(0)\n",
    "    rendered_images = render(Meshes(verts=[sampled_mesh.vertices], faces=[sampled_mesh.faces], verts_normals=[sampled_mesh.vertex_normals]), None, azim, elev, dist, color)[0].squeeze(0)\n",
    "\n",
    "    if n_augs == 0:\n",
    "        clip_image = clip_transform(rendered_images)\n",
    "        encoded_renders = clip_model.encode_image(clip_image)\n",
    "        loss = torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
    "\n",
    "    # Check augmentation steps\n",
    "    if cropsteps != 0 and cropupdate != 0 and i != 0 and i % cropsteps == 0:\n",
    "        curcrop += cropupdate\n",
    "        normaugment_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(res, scale=(curcrop, curcrop)),\n",
    "            transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
    "            clip_normalizer\n",
    "        ])\n",
    "\n",
    "    if n_augs > 0:\n",
    "        loss = 0.0\n",
    "        for _ in range(n_augs):\n",
    "            augmented_image = augment_transform(rendered_images)\n",
    "            encoded_renders = clip_model.encode_image(augmented_image)\n",
    "            if clipavg == \"view\":\n",
    "                if encoded_text.shape[0] > 1:\n",
    "                    loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
    "                                                    torch.mean(encoded_text, dim=0), dim=0)\n",
    "                else:\n",
    "                    loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
    "                                                    encoded_text)\n",
    "            else:\n",
    "                loss -= torch.mean(torch.cosine_similarity(encoded_renders, encoded_text))\n",
    "    if splitnormloss:\n",
    "        for param in mlp.mlp_normal.parameters():\n",
    "            param.requires_grad = False\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "\n",
    "    if n_normaugs > 0:\n",
    "        normloss = 0.0\n",
    "        for _ in range(n_normaugs):\n",
    "            augmented_image = normaugment_transform(rendered_images)\n",
    "            encoded_renders = clip_model.encode_image(augmented_image)\n",
    "            if clipavg == \"view\":\n",
    "                if norm_encoded.shape[0] > 1:\n",
    "                    normloss -= normweight * torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
    "                                                                        torch.mean(norm_encoded, dim=0),\n",
    "                                                                        dim=0)\n",
    "                else:\n",
    "                    normloss -= normweight * torch.cosine_similarity(\n",
    "                        torch.mean(encoded_renders, dim=0, keepdim=True),\n",
    "                        norm_encoded)\n",
    "            else:\n",
    "                normloss -= normweight * torch.mean(\n",
    "                    torch.cosine_similarity(encoded_renders, norm_encoded))\n",
    "        if splitnormloss:\n",
    "            for param in mlp.mlp_normal.parameters():\n",
    "                param.requires_grad = True\n",
    "        if splitcolorloss:\n",
    "            for param in mlp.mlp_rgb.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        normloss.backward(retain_graph=True)\n",
    "\n",
    "    # Also run separate loss on the uncolored displacements\n",
    "    if geoloss:\n",
    "        default_color = torch.zeros(len(mesh.vertices), 3).to(device)\n",
    "        default_color[:, :] = torch.tensor([0.5, 0.5, 0.5]).to(device)\n",
    "        sampled_mesh.face_attributes = default_color\n",
    "\n",
    "        elev = torch.cat((torch.tensor([frontview_center[1]]), torch.randn(n_views - 1) * np.pi / frontview_std + frontview_center[1]))\n",
    "        azim = torch.cat((torch.tensor([frontview_center[0]]), torch.randn(n_views - 1) * 2 * np.pi / frontview_std + frontview_center[0]))\n",
    "        dist = torch.ones((n_views), dtype=torch.float) * render_dist\n",
    "        color = sampled_mesh.face_attributes\n",
    "        azim, elev, dist, color = azim.unsqueeze(0), elev.unsqueeze(0), dist.unsqueeze(0), color.unsqueeze(0)\n",
    "        geo_renders = render(Meshes(verts=[sampled_mesh.vertices], faces=[sampled_mesh.faces], verts_normals=[sampled_mesh.vertex_normals]), None, azim, elev, dist, color)[0].squeeze(0)\n",
    "\n",
    "        if n_normaugs > 0:\n",
    "            normloss = 0.0\n",
    "            ### avgview != aug\n",
    "            for _ in range(n_normaugs):\n",
    "                augmented_image = displaugment_transform(geo_renders)\n",
    "                encoded_renders = clip_model.encode_image(augmented_image)\n",
    "                if norm_encoded.shape[0] > 1:\n",
    "                    normloss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
    "                                                        torch.mean(norm_encoded, dim=0), dim=0)\n",
    "                else:\n",
    "                    normloss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
    "                                                        norm_encoded)\n",
    "\n",
    "            normloss.backward(retain_graph=True)\n",
    "    \n",
    "    optim.step()\n",
    "\n",
    "    for param in mlp.mlp_normal.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in mlp.mlp_rgb.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    if activate_scheduler:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # Adjust normweight if set\n",
    "    if decayfreq is not None:\n",
    "        if i % decayfreq == 0:\n",
    "            normweight *= cropdecay\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        report_process(out_dir, i, loss, loss_check, losses, rendered_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Visualize ouputput Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_final_results(out_dir, losses, mesh, mlp, network_input, vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mvtorchenv1]",
   "language": "python",
   "name": "conda-env-mvtorchenv1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
