{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MVTorch](https://github.com/ajhamdi/mvtorch) Neural Radiance Field Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- install `mvtorch` from [here](https://github.com/ajhamdi/mvtorch/blob/main/INSTALL.md) and activate the environment in the notebook.\n",
    "\n",
    "- download common 3D datasets ([nerf_synthetic](https://drive.google.com/drive/folders/1JDdLGDruGNXWnM1eqY1FNL9PlStjaKWi)) and unzip inside `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && cd .. && mkdir data && cd data/ \n",
    "#gdown --folder 1JDdLGDruGNXWnM1eqY1FNL9PlStjaKWi # download nerf_synthetic from Google Drive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depenenancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from mvtorch.data import load_nerf_data\n",
    "from mvtorch.models.nerf import create_nerf, get_rays_np, get_rays, render, render_path\n",
    "import numpy as np\n",
    "import torch\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '../../data/nerf_synthetic/chair' # Input data directory, specifiy where did you put the data relative to notebook\n",
    "testskip = 8 # Load 1/testskip images from test/val sets\n",
    "white_bkgd = True # Render synthetic data on a white background\n",
    "render_test = False # Render the test set instead of render_poses path\n",
    "N_rand = 32*32 # Batch size (number of random rays per gradient step)\n",
    "no_batching = True # Only take random rays from 1 image at a time\n",
    "i_print = 100 # Frequency of console printout\n",
    "lrate = 5e-4 # Learning rate\n",
    "lrate_decay = 500 # Exponential learning rate decay (in 1000 steps)\n",
    "multires = 10 # log2 of max freq for positional encoding (3D location)\n",
    "i_embed = 0 # Set 0 for default positional encoding, -1 for none\n",
    "use_viewdirs = True # Use full 5D input instead of 3D\n",
    "multires_views = 4 # log2 of max freq for positional encoding (2D direction)\n",
    "N_importance = 128 # Number of additional fine samples per ray\n",
    "netdepth = 8 # Layers in network\n",
    "netwidth = 256 # Channels per layer\n",
    "netdepth_fine = 8 # Layers in fine network\n",
    "netwidth_fine = 256 # Channels per layer in fine network\n",
    "chunk = 1024*32 # Number of rays processed in parallel, decrease if running out of memory\n",
    "netchunk = 1024*64 # Number of pts sent through network in parallel, decrease if running out of memory\n",
    "N_samples = 64 # Number of coarse samples per ray\n",
    "perturb = 1. # Set to 0. for no jitter, 1. for jitter\n",
    "raw_noise_std = 0. # Std dev of noise added to regularize sigma_a output, 1e0 recommended\n",
    "no_ndc = True # do not use normalized device coordinates (set for non-forward facing scenes)\n",
    "lindisp = False # Sampling linearly in disparity rather than depth\n",
    "precrop_iters = 500 # Number of steps to train on central crops\n",
    "precrop_frac = .5 # Fraction of img taken for central crops\n",
    "i_video = 50000 # Frequency of render_poses video saving\n",
    "i_testset = 50000 # Frequency of testset saving\n",
    "basedir = './results/' # Where to store ckpts and logs\n",
    "expname = 'test' # Experiment name\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Set torch device\n",
    "\n",
    "# Misc\n",
    "img2mse = lambda x, y : torch.mean((x - y) ** 2)\n",
    "mse2psnr = lambda x : -10. * torch.log(x) / torch.log(torch.Tensor([10.]))\n",
    "to8b = lambda x : (255*np.clip(x,0,1)).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded nerf data (138, 800, 800, 4) torch.Size([40, 4, 4]) [800, 800, 1111.1110311937682] ../../data/nerf_synthetic/chair\n"
     ]
    }
   ],
   "source": [
    "K = None\n",
    "images, poses, render_poses, hwf, i_split = load_nerf_data(datadir, testskip)\n",
    "print('Loaded nerf data', images.shape, render_poses.shape, hwf, datadir)\n",
    "i_train, i_val, i_test = i_split\n",
    "\n",
    "near = 2.\n",
    "far = 6.\n",
    "\n",
    "if white_bkgd:\n",
    "    images = images[...,:3]*images[...,-1:] + (1.-images[...,-1:])\n",
    "else:\n",
    "    images = images[...,:3]\n",
    "\n",
    "# Cast intrinsics to right types\n",
    "H, W, focal = hwf\n",
    "H, W = int(H), int(W)\n",
    "hwf = [H, W, focal]\n",
    "\n",
    "if K is None:\n",
    "    K = np.array([\n",
    "        [focal, 0, 0.5*W],\n",
    "        [0, focal, 0.5*H],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "if render_test:\n",
    "    render_poses = np.array(poses[i_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NeRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not ndc!\n"
     ]
    }
   ],
   "source": [
    "render_kwargs_train, render_kwargs_test, grad_vars, optimizer = create_nerf(\n",
    "    multires=multires,\n",
    "    i_embed=i_embed,\n",
    "    use_viewdirs=use_viewdirs,\n",
    "    multires_views=multires_views,\n",
    "    N_importance=N_importance,\n",
    "    netdepth=netdepth,\n",
    "    netwidth=netwidth,\n",
    "    device=device,\n",
    "    netdepth_fine=netdepth_fine,\n",
    "    netwidth_fine=netwidth_fine,\n",
    "    lrate=lrate,\n",
    "    netchunk=netchunk,\n",
    "    white_bkgd=white_bkgd,\n",
    "    N_samples=N_samples,\n",
    "    perturb=perturb,\n",
    "    raw_noise_std=raw_noise_std,\n",
    "    no_ndc=no_ndc,\n",
    "    lindisp=lindisp,\n",
    ")\n",
    "global_step = 0\n",
    "\n",
    "bds_dict = {\n",
    "    'near' : near,\n",
    "    'far' : far,\n",
    "}\n",
    "render_kwargs_train.update(bds_dict)\n",
    "render_kwargs_test.update(bds_dict)\n",
    "\n",
    "# Move testing data to GPU\n",
    "render_poses = torch.Tensor(render_poses).to(device)\n",
    "K = torch.Tensor(K).to(device)\n",
    "\n",
    "\n",
    "# Prepare raybatch tensor if batching random rays\n",
    "N_rand = N_rand\n",
    "use_batching = not no_batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rays Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_batching:\n",
    "    # For random ray batching\n",
    "    print('get rays')\n",
    "    rays = np.stack([get_rays_np(H, W, K, p) for p in poses[:,:3,:4]], 0) # [N, ro+rd, H, W, 3]\n",
    "    print('done, concats')\n",
    "    rays_rgb = np.concatenate([rays, images[:,None]], 1) # [N, ro+rd+rgb, H, W, 3]\n",
    "    rays_rgb = np.transpose(rays_rgb, [0,2,3,1,4]) # [N, H, W, ro+rd+rgb, 3]\n",
    "    rays_rgb = np.stack([rays_rgb[i] for i in i_train], 0) # train images only\n",
    "    rays_rgb = np.reshape(rays_rgb, [-1,3,3]) # [(N-1)*H*W, ro+rd+rgb, 3]\n",
    "    rays_rgb = rays_rgb.astype(np.float32)\n",
    "    print('shuffle rays')\n",
    "    np.random.shuffle(rays_rgb)\n",
    "\n",
    "    print('done')\n",
    "    i_batch = 0\n",
    "\n",
    "# Move training data to GPU\n",
    "if use_batching:\n",
    "    images = torch.Tensor(images).to(device)\n",
    "poses = torch.Tensor(poses).to(device)\n",
    "if use_batching:\n",
    "    rays_rgb = torch.Tensor(rays_rgb).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NeRF Optimzation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin\n",
      "TRAIN views are [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "TEST views are [113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130\n",
      " 131 132 133 134 135 136 137]\n",
      "VAL views are [100 101 102 103 104 105 106 107 108 109 110 111 112]\n",
      "[Config] Center cropping of size 400 x 400 is enabled until iter 500\n",
      "[TRAIN] Iter: 100 Loss: 0.13307276368141174  PSNR: 11.514184951782227\n",
      "[TRAIN] Iter: 200 Loss: 0.08024056255817413  PSNR: 13.54585075378418\n",
      "[TRAIN] Iter: 300 Loss: 0.08390150964260101  PSNR: 13.743597984313965\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 55\u001b[0m\n\u001b[1;32m     51\u001b[0m         target_s \u001b[38;5;241m=\u001b[39m target[select_coords[:, \u001b[38;5;241m0\u001b[39m], select_coords[:, \u001b[38;5;241m1\u001b[39m]]  \u001b[38;5;66;03m# (N_rand, 3)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#####  Core optimization loop  #####\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m rgb, disp, acc, extras \u001b[38;5;241m=\u001b[39m \u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrays\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_rays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrender_kwargs_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     60\u001b[0m img_loss \u001b[38;5;241m=\u001b[39m img2mse(rgb, target_s)\n",
      "File \u001b[0;32m~/anaconda3/envs/mvtorchenv1/lib/python3.9/site-packages/mvtorch/models/nerf.py:493\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(H, W, K, chunk, rays, c2w, ndc, near, far, use_viewdirs, c2w_staticcam, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m     rays \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([rays, viewdirs], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# Render and reshape\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m all_ret \u001b[38;5;241m=\u001b[39m \u001b[43mbatchify_rays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m all_ret:\n\u001b[1;32m    495\u001b[0m     k_sh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(sh[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(all_ret[k]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[0;32m~/anaconda3/envs/mvtorchenv1/lib/python3.9/site-packages/mvtorch/models/nerf.py:426\u001b[0m, in \u001b[0;36mbatchify_rays\u001b[0;34m(rays_flat, chunk, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m all_ret \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, rays_flat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], chunk):\n\u001b[0;32m--> 426\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mrender_rays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrays_flat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m ret:\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_ret:\n",
      "File \u001b[0;32m~/anaconda3/envs/mvtorchenv1/lib/python3.9/site-packages/mvtorch/models/nerf.py:403\u001b[0m, in \u001b[0;36mrender_rays\u001b[0;34m(ray_batch, network_fn, network_query_fn, N_samples, retraw, lindisp, perturb, N_importance, network_fine, white_bkgd, raw_noise_std, verbose, pytest)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m#         raw = run_network(pts, fn=run_fn)\u001b[39;00m\n\u001b[1;32m    401\u001b[0m         raw \u001b[38;5;241m=\u001b[39m network_query_fn(pts, viewdirs, run_fn)\n\u001b[0;32m--> 403\u001b[0m         rgb_map, disp_map, acc_map, weights, depth_map \u001b[38;5;241m=\u001b[39m \u001b[43mraw2outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrays_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_noise_std\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhite_bkgd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpytest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpytest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m     ret \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_map\u001b[39m\u001b[38;5;124m'\u001b[39m : rgb_map, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp_map\u001b[39m\u001b[38;5;124m'\u001b[39m : disp_map, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc_map\u001b[39m\u001b[38;5;124m'\u001b[39m : acc_map}\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retraw:\n",
      "File \u001b[0;32m~/anaconda3/envs/mvtorchenv1/lib/python3.9/site-packages/mvtorch/models/nerf.py:278\u001b[0m, in \u001b[0;36mraw2outputs\u001b[0;34m(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest)\u001b[0m\n\u001b[1;32m    275\u001b[0m raw2alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m raw, dists, act_fn\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mrelu: \u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mact_fn(raw)\u001b[38;5;241m*\u001b[39mdists)\n\u001b[1;32m    277\u001b[0m dists \u001b[38;5;241m=\u001b[39m z_vals[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m-\u001b[39m z_vals[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 278\u001b[0m dists \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([dists, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1e10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdists\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdists\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [N_rays, N_samples]\u001b[39;00m\n\u001b[1;32m    280\u001b[0m dists \u001b[38;5;241m=\u001b[39m dists \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(rays_d[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;28;01mNone\u001b[39;00m,:], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    282\u001b[0m rgb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(raw[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,:\u001b[38;5;241m3\u001b[39m])  \u001b[38;5;66;03m# [N_rays, N_samples, 3]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_iters = 200000 + 1\n",
    "print('Begin')\n",
    "print('TRAIN views are', i_train)\n",
    "print('TEST views are', i_test)\n",
    "print('VAL views are', i_val)\n",
    "\n",
    "start = global_step + 1\n",
    "for i in range(start, N_iters):\n",
    "    # Sample random ray batch\n",
    "    if use_batching:\n",
    "        # Random over all images\n",
    "        batch = rays_rgb[i_batch:i_batch+N_rand] # [B, 2+1, 3*?]\n",
    "        batch = torch.transpose(batch, 0, 1)\n",
    "        batch_rays, target_s = batch[:2], batch[2]\n",
    "\n",
    "        i_batch += N_rand\n",
    "        if i_batch >= rays_rgb.shape[0]:\n",
    "            print(\"Shuffle data after an epoch!\")\n",
    "            rand_idx = torch.randperm(rays_rgb.shape[0])\n",
    "            rays_rgb = rays_rgb[rand_idx]\n",
    "            i_batch = 0\n",
    "\n",
    "    else:\n",
    "        # Random from one image\n",
    "        img_i = np.random.choice(i_train)\n",
    "        target = images[img_i]\n",
    "        target = torch.Tensor(target).to(device)\n",
    "        pose = poses[img_i, :3,:4]\n",
    "        if N_rand is not None:\n",
    "            rays_o, rays_d = get_rays(H, W, K, pose)  # (H, W, 3), (H, W, 3)\n",
    "\n",
    "            if i < precrop_iters:\n",
    "                dH = int(H//2 * precrop_frac)\n",
    "                dW = int(W//2 * precrop_frac)\n",
    "                coords = torch.stack(\n",
    "                    torch.meshgrid(\n",
    "                        torch.linspace(H//2 - dH, H//2 + dH - 1, 2*dH), \n",
    "                        torch.linspace(W//2 - dW, W//2 + dW - 1, 2*dW)\n",
    "                    ), -1)\n",
    "                if i == start:\n",
    "                    print(f\"[Config] Center cropping of size {2*dH} x {2*dW} is enabled until iter {precrop_iters}\")                \n",
    "            else:\n",
    "                coords = torch.stack(torch.meshgrid(torch.linspace(0, H-1, H), torch.linspace(0, W-1, W)), -1)  # (H, W, 2)\n",
    "\n",
    "            coords = torch.reshape(coords, [-1,2])  # (H * W, 2)\n",
    "            select_inds = np.random.choice(coords.shape[0], size=[N_rand], replace=False)  # (N_rand,)\n",
    "            select_coords = coords[select_inds].long()  # (N_rand, 2)\n",
    "            rays_o = rays_o[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n",
    "            rays_d = rays_d[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n",
    "            batch_rays = torch.stack([rays_o, rays_d], 0)\n",
    "            target_s = target[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n",
    "\n",
    "    \n",
    "    #####  Core optimization loop  #####\n",
    "    rgb, disp, acc, extras = render(H, W, K, chunk=chunk, rays=batch_rays,\n",
    "                                            verbose=i < 10, retraw=True,\n",
    "                                            **render_kwargs_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    img_loss = img2mse(rgb, target_s)\n",
    "    trans = extras['raw'][...,-1]\n",
    "    loss = img_loss\n",
    "    psnr = mse2psnr(img_loss.cpu())\n",
    "\n",
    "    if 'rgb0' in extras:\n",
    "        img_loss0 = img2mse(extras['rgb0'], target_s)\n",
    "        loss = loss + img_loss0\n",
    "        psnr0 = mse2psnr(img_loss0.cpu())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # NOTE: IMPORTANT!\n",
    "    ###   update learning rate   ###\n",
    "    decay_rate = 0.1\n",
    "    decay_steps = lrate_decay * 1000\n",
    "    new_lrate = lrate * (decay_rate ** (global_step / decay_steps))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lrate\n",
    "    ################################\n",
    "\n",
    "\n",
    "    if i % i_video == 0 and i > 0:\n",
    "        if not os.path.exists(os.path.join(basedir, expname)):\n",
    "            os.makedirs(os.path.join(basedir, expname))\n",
    "        # Turn on testing mode\n",
    "        with torch.no_grad():\n",
    "            rgbs, disps = render_path(render_poses, hwf, K, chunk, render_kwargs_test)\n",
    "        print('Done, saving', rgbs.shape, disps.shape)\n",
    "        moviebase = os.path.join(basedir, expname, '{}_spiral_{:06d}_'.format(expname, i))\n",
    "        imageio.mimwrite(moviebase + 'rgb.mp4', to8b(rgbs), fps=30, quality=8)\n",
    "        imageio.mimwrite(moviebase + 'disp.mp4', to8b(disps / np.max(disps)), fps=30, quality=8)\n",
    "    \n",
    "    if i % i_testset == 0 and i > 0:\n",
    "        if not os.path.exists(os.path.join(basedir, expname)):\n",
    "            os.makedirs(os.path.join(basedir, expname))\n",
    "        testsavedir = os.path.join(basedir, expname, 'testset_{:06d}'.format(i))\n",
    "        os.makedirs(testsavedir, exist_ok=True)\n",
    "        print('test poses shape', poses[i_test].shape)\n",
    "        with torch.no_grad():\n",
    "            render_path(torch.Tensor(poses[i_test]).to(device), hwf, K, chunk, render_kwargs_test, gt_imgs=images[i_test], savedir=testsavedir)\n",
    "        print('Saved test set')\n",
    "\n",
    "    if i % i_print == 0:\n",
    "        print(f\"[TRAIN] Iter: {i} Loss: {loss.item()}  PSNR: {psnr.item()}\")\n",
    "\n",
    "    global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mvtorchenv1]",
   "language": "python",
   "name": "conda-env-mvtorchenv1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
